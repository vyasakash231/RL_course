{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Monte-Carlo method it is necessary to wait until the end of the episode before updating the value function. This is a serious problem because some applications can have very long episodes with learning delayed to the end of each one. Moreover, in some environments the completion of the episode is not guaranteed. Hence to solve this problem we'll use **Temporal-Difference method**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal-Difference(0) based Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-Value matrix after 50000 iterations:\n",
      "[[ 0.84578261  0.90616525  0.95581157  1.        ]\n",
      " [ 0.796158    0.          0.69817666 -1.        ]\n",
      " [ 0.73362195  0.68367526  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "np.set_printoptions(suppress=True)  # to aviod scientific notation while printing numpy array\n",
    "sys.path.append('..')\n",
    "\n",
    "from environment import Gridworld\n",
    "\n",
    "env = Gridworld()\n",
    "gamma = 0.999 # discount factor\n",
    "alpha = 0.01\n",
    "\n",
    "V = np.zeros((3,4)) # Defining an empty state-value matrix\n",
    "\n",
    "# INPUT -> Optimal policy using value iteration, {up=0, left=1, down=2, right=3}\n",
    "policy = np.array([[3,      3, 3, -1],\n",
    "                   [0, np.NaN, 0, -1],\n",
    "                   [0,      1, 1,  1]])\n",
    "\n",
    "tot_episodes = 50000 # no of eposides\n",
    "time_step_for_task = 20 # time steps required to perform a task in one episode\n",
    "\n",
    "for current_episode in range(tot_episodes):\n",
    "    robot_current_state = env.reset() # Reset and return the first observation\n",
    "\n",
    "    # Now we can run for some time step within the episode\n",
    "    for step_time in range(time_step_for_task): \n",
    "        action = policy[robot_current_state[0], robot_current_state[1]]\n",
    "        action_applied = np.random.choice(4, 1, p=env.action_transition_matrix[int(action),:]) # Generate a non-uniform random sample (following p-distribution) from np.arange(4) of size 1\n",
    "        \n",
    "        next_position = env.step(action) # perform this action\n",
    "        \n",
    "        state_reward = env.reward[next_position[0], next_position[1]] # state reward obtained (for state at t+1)\n",
    "\n",
    "        # One-step TD(0) update\n",
    "        V_current_xk = V[robot_current_state[0], robot_current_state[1]] # V(x_{k}) -> current\n",
    "        V_current_xk1 = V[next_position[0], next_position[1]] # V(x_{k+1}) -> current\n",
    "        rk1 = state_reward # r_{k+1}\n",
    "\n",
    "        V[robot_current_state[0],robot_current_state[1]] = V_current_xk + alpha*(rk1 + gamma*V_current_xk1 - V_current_xk) # V(x_{k}) -> new\n",
    "\n",
    "        robot_current_state = next_position # Update robot current position (to observation at t+1)\n",
    "\n",
    "        if env.state_matrix[next_position[0], next_position[1]] == 1:\n",
    "            break\n",
    "\n",
    "    \n",
    "V[np.where(env.reward == 1)] = 1\n",
    "V[np.where(env.reward == -1)] = -1\n",
    "\n",
    "# Time to check the utility matrix obtained\n",
    "print(\"State-Value matrix after \" + str(tot_episodes) + \" iterations:\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal-Difference($\\lambda$) based Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-Value matrix after 50000 iterations:\n",
      "[[ 0.86346025  0.92387343  0.97134743  1.        ]\n",
      " [ 0.81242462  0.          0.78939655 -1.        ]\n",
      " [ 0.75046207  0.68465266  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "np.set_printoptions(suppress=True)  # to aviod scientific notation while printing numpy array\n",
    "sys.path.append('..')\n",
    "\n",
    "from environment import Gridworld\n",
    "\n",
    "env = Gridworld()\n",
    "gamma = 0.999 # discount factor\n",
    "alpha = 0.01\n",
    "lambda_new = 0.5\n",
    "\n",
    "V = np.zeros((3,4)) # Defining an empty state-value matrix\n",
    "\n",
    "#Define and print the eligibility trace matrix\n",
    "trace_matrix = np.zeros((3,4))\n",
    "\n",
    "# INPUT -> Optimal policy using value iteration, {up=0, left=1, down=2, right=3}\n",
    "policy = np.array([[3,      3, 3, -1],\n",
    "                   [0, np.NaN, 0, -1],\n",
    "                   [0,      1, 1,  1]])\n",
    "\n",
    "tot_episodes = 50000 # no of eposides\n",
    "time_step_for_task = 1000 # time steps required to perform a task in one episode\n",
    "\n",
    "for current_episode in range(tot_episodes):\n",
    "    robot_current_state = env.reset() # Reset and return the first observation\n",
    "\n",
    "    # Now we can run for some time step within the episode\n",
    "    for step_time in range(time_step_for_task): \n",
    "        action = policy[robot_current_state[0], robot_current_state[1]]\n",
    "\n",
    "        next_position = env.step(action) # perform this action\n",
    "        \n",
    "        state_reward = env.reward[next_position[0], next_position[1]] # state reward obtained (for state at t+1) -> r_{k+1}\n",
    "\n",
    "        # One-step TD(lambda) update\n",
    "        V_current_xk = V[robot_current_state[0], robot_current_state[1]] # V(x_{k}) -> current\n",
    "        V_current_xk1 = V[next_position[0], next_position[1]] # V(x_{k+1}) -> current\n",
    "        \n",
    "        delta = state_reward + gamma*V_current_xk1 - V_current_xk # TD error\n",
    "\n",
    "        trace_matrix[robot_current_state[0], robot_current_state[1]] += 1 # Adding +1 in the trace matrix (only the state visited)\n",
    "\n",
    "        V += alpha*delta*trace_matrix # V(x_{k}) -> new\n",
    "\n",
    "        trace_matrix = gamma*lambda_new*trace_matrix # updating trace_matrix\n",
    "\n",
    "        robot_current_state = next_position # Update robot current position (to observation at t+1)\n",
    "\n",
    "        if env.state_matrix[next_position[0], next_position[1]] == 1:\n",
    "            break\n",
    "\n",
    "V[np.where(env.reward == 1)] = 1\n",
    "V[np.where(env.reward == -1)] = -1\n",
    "\n",
    "# Time to check the utility matrix obtained\n",
    "print(\"State-Value matrix after \" + str(tot_episodes) + \" iterations:\")\n",
    "print(V) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
