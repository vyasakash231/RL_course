{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REINFORCE (Monte Carlo policy gradient)\n",
    "* Instead of using Deep Neural Network to approximate Q-value function, they learned the values of actions and then selected actions based on their estimated action values.we can directly learn a suitable policy $\\pi$ parametrizing by $\\theta$.\n",
    "So, shift from indirect value based approach: \n",
    "$$\\hat{q}(x,u,\\bold{w}) \\simeq q(x,u)$$\n",
    "to, direct policy-based solution:\n",
    "$$\\pi(u|x) = \\mathbb{P}[U=u|X=x] \\simeq \\pi(u|x,\\theta)$$\n",
    "where, $\\theta \\in \\mathbb{R}^d$ is the policy parameter vector.\n",
    "\n",
    "**Policy Object Function**\n",
    "* Goal: find optimal $\\theta^{*}$ given policy $\\pi(u|x,\\theta)$\n",
    "* Problem: which measure of optimality should be used?\n",
    "\n",
    "**Policy Optimization**\n",
    "* We consider methods for learning the policy parameter $\\theta$ based on the gradient of a scalar performance measure $J(\\theta)$ with respect to the policy parameter $\\theta$.\n",
    "* we can not guarantee, a global optimal soln for such a problem, but we can ignore this and still try to solve the problem.\n",
    "* These methods seek to maximize performance, so their updates approximate gradient ascent in $J$:\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha*J(\\theta_{t})$$\n",
    "* All methods that follow this general schema we call *Policy Gradient Methods*.\n",
    "\n",
    "**Possible Optimality Metrices** \\\n",
    "In *policy gradient methods*, the policy can be parameterized in any way, as long as $\\pi(u|x,\\theta)$ is differentiable with respect to its parameters $\\theta$, that is, as long as $\\nabla{\\pi}(u|x,\\theta)$ exists and is finite for all $x \\in \\chi$, $u \\in A(x)$, and $\\theta \\in \\mathbb{R}^d$.\n",
    "1. state value (in episodic tasks), here $x_0$ is the start state,\n",
    "$$J(\\theta) = v_{\\pi_{\\theta}}(x_0) = \\mathbb{E}[v|X=x_0, \\theta]$$\n",
    "2. average reward (in continuous tasks)\n",
    "$$J(\\theta) = \\bar{r}_{\\pi_{\\theta}}$$\n",
    "\n",
    "![](../Images/pgt_proof.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../Images/reinforce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
